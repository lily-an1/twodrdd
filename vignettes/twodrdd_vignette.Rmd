---
title: "Introduction to twodrdd"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to twodrdd}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r, include = FALSE}
library( tidyverse )
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
theme_set( theme_minimal() )

library(tidyverse)
knitr::opts_chunk$set(echo = TRUE,
          fig.width = 6,
          fig.height = 4,
          out.width = "85%",
           message = FALSE,
          fig.align = "center")
options(list(dplyr.summarise.inform = FALSE))
theme_set( theme_classic() )

```

```{r setup, include=FALSE}
library(twodrdd)
```

The `twodrdd` package allows users to estimate boundary average treatment effects for two-dimensional regression discontinuity designs (RDDs) where a singular treatment is defined by scores on two running variables. The package implements the following: two nonparametric surface response methods (Gaussian process regression, as implemented in the `laGP` [@gramacy2016laGP]
package, and loess), and parametric linear and quadratic surface response methods.
The package also provides the binding score method, and the pooled frontier method [@wong2013analyzing], as implemented by the `rddapp` package [@rddapp2021].

In particular, this package will analyze data of the following form:

```{r}
# Using example data
data(fakeData)
head(fakeData)
```
Our boundaries carve out a corner of the data.
These data have been centered so the cut points are already at 0.

```{r}
ggplot( fakeData, aes( rating1, rating2, col = as.factor(T) ) ) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(title = "Example data for twodrdd package",
       color = "Treatment" )
```


# Getting started

To use the twodrdd package, ensure you have the following variables in your data, as in the illustration above:

-   T: Treatment variable (a 0/1 indicator)
-   rating1: Running variable 1
-   rating2: Running variable 2
-   Y: Outcome

**NOTE:** This package works for two-dimensional RDD setups where the treated units are defined by both running variables being **greater than or equal to** their cutoffs.
In some 2D RDD contexts, the control units would be defined by this definition---if so, then all estimated treatment effects produced by this package will need to be multiplied by -1.


# Compare two-dimensional RDD methods: use analysis()

If you want to compare different RDD methods, our `analysis()` function allows you to pick and choose from several options (see the paper for details):

-   Gaussian process regression
-   Residualized Gaussian process regression - our preferred implementation
-   Loess
-   Parametric linear surface response
-   Parametric quadratic surface response
-   Binding score
-   Pooled frontier
-   OLS - This fits the simple model \$ Y = rating1 + rating2 + T \$. This was mostly a check for us that the function was running properly, and is not recommended for use.

There are some other parameters to point out:

-   cut1 and cut2: These are the running variable cutoffs and are important if you are bringing your own data. They default to 0, in line with the simulation's data generating process. However, if you are using the analysis function with your own data, pass the running variable cutoffs as parameters here so that the running variables can be centered before the methods are run.
-   n_sentinel: The number of sentinels (per running variable) you wish to use, for those methods that average along the boundary. We used n_sentinel = 20 in our simulation, which meant 39 sentinels total (since 2 overlap).
-   startnum and endnum: These are passed to the `laGP` package and are relevant if the Gaussian process regression type used is local approximate (the function `laGP`), which is faster to fit (but less accurate) than the full fitting method.

To compare the binding score, pooled frontier, residualized Gaussian process regression, and loess methods simply call `analysis()`.
Here we do it on some fake data provided in the package designed to mimic the empirical example of the paper:

```{r}
data( fakeData )
analysis(dat = fakeData, 
         cut1=0, cut2=0,
         n_sentinel = 20,
         startnum=50, endnum=100,
         include_OLS = FALSE,
         include_BINDING = TRUE,
         include_FRONTIER = TRUE,
         include_GP = FALSE,
         include_GP_RES = TRUE,
         include_loess = TRUE,
         include_PARA_LINEAR = FALSE,
         include_PARA_QUAD = FALSE)
```

Our output has spit out the estimates and standard errors from using binding score and residualized Gaussian process regression. The "AFE_wt parameter" refers to density-weighted residualized Gaussian process regression, and "AFE_prec" refers to precision-weighted residualized Gaussian process regression. There are two variables related to sample size in the output: "n" tells us how many units were in the data, and "sampsize" (which is specific to OLS, binding score, and pooled frontier) tells us how many were used.

# Looking at the sentinels

You can look at the estimated impacts along the sentinel points for a given dataset for the Gaussian Process, if desired:

```{r}
gpres <- gaussianp(sampdat=fakeData,
                   residualize = TRUE,
                   n_sentinel = 20 )
names(gpres)
gpres %>%
  dplyr::select( sentNum:rating2, estimate, se, weight ) %>%
  knitr::kable( digits = 2 )
```
Note that it dropped very low weight sentinels.

We can plot the estimated curve:

```{r}
ggplot( gpres, aes( sentNum, estimate ) ) +
  geom_line() +
  geom_point() +
  geom_errorbar(aes(ymin = estimate - 1.96 * se, 
                    ymax = estimate + 1.96 * se), width = 0 ) +
  labs(title = "Estimated treatment effect along sentinels",
       x = "Sentinel number",
       y = "Estimated treatment effect")
```

# Generating fake data

Here we show how to use our data generator, gen_dat_sim() (that we also use in the simulation of our paper). `gen_dat_sim()` borrows heavily from the data generating code in @porter2017estimating.

```{r}
dat = gen_dat_sim( sim = 1, cut1 = 0.2, cut2 = 0.3, 
                   n = 700, rho = 0.80, s = 1 , seed = 847)

names(dat)

# The data itself
head( dat$data )
```

The DGP generator provides the data along with some parameters and values that can be useful for simulation.
Each call will create `s` data sets each of size `n` that have two running variables ("rating1" and "rating2") with "rho" correlation between them; these are bivariate standard Gaussian.
Do not forget to include where the cutoffs are via `cut1` and `cut2`).
For this method, the cut values are going to be values from 0-1 that indicates the *percentile* at which the cutoff lies on the running variable. We can also set a seed for replicability.

We have different options for the type of simulated data to create (distinguished by the parameter "sim"). For specifics, see Table 1 in the paper which discusses sim types of 1-4. A high-level way to think about these types of data is that sim = 1 data are very simple: they have a constant treatment effect and are linear. From sim = 2 to sim = 4 the data get more complex and heterogeneous. We have two other data options in our data generating function: sim = 5 data have an increasing treatment effect along only one running variable, and sim = 6 data have no treatment effect whatsoever.


# References
